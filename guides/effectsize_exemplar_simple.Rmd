## Exemplar: Simple effect size {#effectsize_exemplar_simple}

<mark>
This section is in *alpha*. We welcome help and feedback at all levels!
If you would like to contribute, please see
[Contributing to the Guidelines](https://github.com/transparentstats/guidelines/wiki/Contributing-to-the-Guidelines).
</mark>

### Libraries needed for this analysis

```{r es-simple-setup, warning = FALSE, message = FALSE}
library(tidyverse)
library(forcats)    # for fct_...()
library(broom)      # for tidy()
library(ggstance)   # for geom_pointrangeh(), stat_summaryh()
```

```{r simple-boilerplate, include = FALSE}
format_num <- function(nums, sigdigits = 3) gsub("\\.$", "", formatC(nums, sigdigits, format = "fg", flag="#"))
```


### Data

Imagine a between-subjects design, with completion time (in milliseconds) measured in two groups, `A` and `B`, with 20 subjects each.

```{r es-simple-data_generation}
set.seed(12)
n <- 20
data <- tibble(
  group = rep(c("A", "B"), each = n),
  completion_time_ms = c(
    rlnorm(n, meanlog = log(170), sdlog = 0.3),
    rlnorm(n, meanlog = log(50), sdlog = 0.4)
  )
)
```

We assume a log-normal model of completion times, which is a commonly-used model of completion time [@Sauro2010] and ensures completion times are all positive.

A good first step in any analysis is always to visualize the data:

```{r es-simple-data_plot, fig.height = 2, fig.width = 4}
p_data <-  # save for the teaser figure
  data %>% 
  ggplot(aes(x = completion_time_ms)) +
  geom_dotplot(binwidth=5) +
  stat_summaryh(aes(y = 0, xintercept = ..x..), fun.x = mean, geom = "vline", color = "red", linetype = "dashed") +
  facet_grid(group ~ ., switch="y") +
  scale_y_continuous(breaks = NULL) +
  geom_vline(xintercept = 0) +
  xlab("Completion time (ms)") +
  ylab("Group")
p_data
```

This plot shows all observed completion times in each group (black dots) along with the mean in each group (dashed red lines).

### Calculating simple effect size

Since we have meaningful units (milliseconds), we will use the *difference* in mean completion time as our effect size. Following [our recommendations on how to report effect size](#effectsize_faq_how_reporting), we also need to report the uncertainty around the sample effect size.

There are several possible approaches to estimating the uncertainty in the difference between the two groups. For simplicity, we show one possible approach in this exemplar, but we provide a non-exhaustive comparison of a few other approaches in the [effect size guideline appendix](#appendix_effectsize_simple).

### Difference in means with Student's t confidence interval

While the response distributions are non-normal, the sampling distribution of the difference in means will still be defined on $(-\infty, +\infty)$ and approximately symmetrical (per the central limit theorem), so we can compute a *Student's t distribution confidence interval* for the difference in means.

```{r es-simple-t_test}
t_result <- 
  t.test(completion_time_ms ~ group, data = data) %>%
  tidy()    # put result in tidy tabular format
t_result
```

The `tidy()`ed output of the `t.test()` function includes an estimate of the mean difference in milliseconds (`estimate`) as well as the lower (`conf.low`) and upper (`conf.high`) bounds of the 95% confidence interval. 
95% of all intervals constructed in this manner will contain the "true" (population) value. 
A 95% t-confidence interval is therefore a reasonable measure of uncertainty about our measured effect size. 
The wider the confidence interval, the less certain we are about what we expect our true effect size to be.

### Reporting simple effect size

Ideally, we would have space in our paper to report the effect size graphically:

```{r es-simple-ci_plot, fig.height = 1, fig.width = 5}
p_simple_effect_size <-   # save for the teaser figure
  t_result %>% 
  ggplot(aes(y = "A - B", x = estimate, xmin = conf.low, xmax = conf.high)) +
  geom_pointrangeh() +
  geom_vline(xintercept = 0, linetype="dashed") +
  xlab("Mean difference in completion time (ms) with 95% CI") +
  ylab("")
p_simple_effect_size
```

This graphical report includes all of the [elements of an effect size report that we recommend](#effectsize_faq_how_reporting):

- The direction of the difference (indicated by the label `A - B`)
- The type of estimate reported (mean difference)
- The type of uncertainty indicated (95% CI)
- The units (ms)

Space may not always permit the graphical report. While it can be less easy to interpret, an alternative is a textual report. **Such a report should still include all of the four elements listed above.** For example:

> Group `A` had a greater mean completion time than group `B` by `r format_num(t_result$estimate, 2)` milliseconds (95% CI: [`r format_num(t_result$conf.low, 2)`, `r format_num(t_result$conf.high, 2)`]).


### Interpreting effect size: same result, different domains = different interpretations

Because simple effect sizes include units, we can use our expert judgment to interpret the report. Authors may wish to do so in order to put their result in context. Because the report above includes everything necessary for other experts to come to their own conclusion, providing our own interpretation does not prevent readers from applying their own judgment and coming to different conclusions.

To illustrate the effect of domain on interpreting effect size, we will imagine two different domains that might have led to the same result reported above, and write a different interpretation of the data for each.


#### Domain 1: Physical prototyping

Imagine the above study was from the comparison of a novel physical user interface prototyping system (treatment `B`) to the previous state of the art (`A`), and the completion time referred to the time for feedback to be given to the user after they perform an input action. We might report the following interpretation of the results:

> Technique `B` offers a **large** improvement in feedback time (~`r format_num(t_result$conf.low, 2)` -- `r format_num(t_result$conf.high, 2)`ms mean decrease), resulting in feedback times that tend to be less than the threshold of human perception (less than about 100ms). By contrast, the larger feedback times offered by technique `A` tended to be above that threshold, possibly degrading users' experience of the prototypes built using that technique.


#### Domain 2: Chatbots

Imagine the same quantitative results, now in the context of a natural language chat bot designed to answer users' questions. Here, technique `A` will be the novel system, with improved natural language capabilities compared to the previous state-of-the-art technique, `B`. We might report the following interpretation of the results:

> While technique `A` takes longer to respond to chat messages (~`r format_num(t_result$conf.low, 2)`--`r format_num(t_result$conf.high, 2)`ms increase in mean response time), we believe this difference is acceptable in the context of an asynchronous chat interface in which users do not expect instantaneous responses. When weighed against the improved natural language capabilites of technique `A`, we believe this **small** increase in response time for messages is worth the improved message content.

The same effect size is plausibly described as **large** in domain 1 and **small** in domain 2, illustrating the importance of expert interpretation to reporting and understanding effect size and the difficulty in applying pre-defined thresholds across domains.

```{r es-simple-teaser_figure, include = FALSE}
library(Cairo)  # for ggsave() that don't use unicode character for points
ggsave("figures/effectsize/p_data.pdf", p_data, height = 1, width = 3, device = cairo_pdf)
ggsave("figures/effectsize/p_simple_effect_size.pdf", p_simple_effect_size, height = 1, width = 3, device = cairo_pdf)
```
